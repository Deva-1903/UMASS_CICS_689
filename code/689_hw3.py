# -*- coding: utf-8 -*-
"""689_HW3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y-XPlyEEyYkLOuBJ7_CktEKhYJx1mM7C
"""
# --- Auto-install required packages ---
def ensure_package(pkg, extra_args=None):
    """Check if a package is installed; if not, install it."""
    try:
        __import__(pkg)
    except ImportError:
        print(f"Installing {pkg}... please wait.")
        cmd = [sys.executable, "-m", "pip", "install", "--quiet", "--upgrade", pkg]
        if extra_args:
            cmd.extend(extra_args)
        subprocess.check_call(cmd)
        __import__(pkg)

# Install dependencies automatically if missing
ensure_package("jax", ["-f", "https://storage.googleapis.com/jax-releases/jax_releases.html"])
ensure_package("pandas")
ensure_package("matplotlib")
ensure_package("numpy")

# --- Safe imports after ensuring installation ---
import os
import sys        # ← ADD THIS LINE
import subprocess
import jax
import jax.numpy as jnp
from jax import grad
from jax.scipy.special import logsumexp
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

print("✅ All dependencies installed successfully!\n")
   
"""
***1.3 Verification by Autodiff***
"""

import jax
import jax.numpy as jnp

def backward_logsumexp(x, u):
    # f(x) = u * logsumexp(x)
    def f(x):
        return u * jax.scipy.special.logsumexp(x)
    return jax.grad(f)(x)

x = jnp.array([1.0, 2.0, 3.0])
u = 0.7
g = backward_logsumexp(x, u)
print("grad", g)
print("difference", u * jax.nn.softmax(x))

"""***2.1 Implementing the Op Class***"""

import numpy as np
from dataclasses import dataclass

@dataclass
class Op:
    name: str
    num_inputs: int
    forward_fn: callable
    backward_fn: callable  # backward_fn(parents_values, upstream) -> tuple of grads

    def __call__(self, *parents_values):
        assert len(parents_values) == self.num_inputs
        out = self.forward_fn(*parents_values)
        def bprop(upstream):
            return self.backward_fn(parents_values, upstream)
        return out, bprop


def _softmax(x):
    m = np.max(x)
    ex = np.exp(x - m)
    return ex / np.sum(ex)


add = Op(
    name="add",
    num_inputs=2,
    forward_fn=lambda x, y: x + y,
    backward_fn=lambda p, u: (u, u)
)

sub = Op(
    name="sub",
    num_inputs=2,
    forward_fn=lambda x, y: x - y,
    backward_fn=lambda p, u: (u, -u)
)

mul = Op(
    name="mul",
    num_inputs=2,
    forward_fn=lambda x, y: x * y,
    backward_fn=lambda p, u: (u * p[1], u * p[0])
)

matmul = Op(
    name="matmul",
    num_inputs=2,
    forward_fn=lambda X, Y: X @ Y,
    backward_fn=lambda p, U: (U @ p[1].T, p[0].T @ U)
)

inner = Op(
    name="inner",
    num_inputs=2,
    forward_fn=lambda x, y: float(x.T @ y),
    backward_fn=lambda p, u: (u * p[1], u * p[0])
)

sum_all = Op(
    name="sum",
    num_inputs=1,
    forward_fn=lambda x: float(np.sum(x)),
    backward_fn=lambda p, u: (u * np.ones_like(p[0]),)
)

solve = Op(
    name="solve",
    num_inputs=2,
    forward_fn=lambda A, x: np.linalg.solve(A, x),
    backward_fn=lambda p, u: (
        -(np.linalg.solve(p[0].T, u)[:, None] @ np.linalg.solve(p[0], p[1])[None, :]),
         np.linalg.solve(p[0].T, u)
    )
)

logdet = Op(
    name="logdet",
    num_inputs=1,
    forward_fn=lambda A: float(np.linalg.slogdet(A)[1]),
    backward_fn=lambda p, u: (u * np.linalg.inv(p[0]).T,)
)

logsumexp = Op(
    name="logsumexp",
    num_inputs=1,
    forward_fn=lambda x: float(np.log(np.sum(np.exp(x - np.max(x)))) + np.max(x)),
    backward_fn=lambda p, u: (u * _softmax(p[0]),)
)

exp = Op(
    name="exp",
    num_inputs=1,
    forward_fn=lambda x: np.exp(x),
    backward_fn=lambda p, u: (u * np.exp(p[0]),)
)

log = Op(
    name="log",
    num_inputs=1,
    forward_fn=lambda x: np.log(x),
    backward_fn=lambda p, u: (u / p[0],)
)


x = np.array([1.0, 2.0, 3.0])
y = np.array([2.0, -1.0, 0.5])
A = np.array([[2.0, 1.0, 0.0],
              [1.0, 3.0, 1.0],
              [0.0, 1.0, 2.0]])
u_scalar = 1.0
u_vec = np.ones_like(x)

ops_to_test = [
    ("add", add, (x, y), u_vec),
    ("mul", mul, (x, y), u_vec),
    ("exp", exp, (x,), u_vec),
    ("log", log, (x,), u_vec),
    ("inner", inner, (x, y), u_scalar),
    ("logsumexp", logsumexp, (x,), u_scalar),
    ("solve", solve, (A, x), u_vec),
    ("logdet", logdet, (A,), u_scalar)
]

print(f"{'Operation':<12} {'Forward Output':<35} {'Backward Output'}")
print("-"*85)

for name, op, args, u in ops_to_test:
    out, bprop = op(*args)
    grads = bprop(u)
    print(f"{name:<12} {str(out):<35} {str(grads)}")

"""***2.2 Implementing the Var Class***"""

import numpy as np
from dataclasses import dataclass

class Var:
    def __init__(self, value, op=None, parents=(), bprop=None, name=None):
        self.value = np.array(value)

        self.op = op

        self.parents = parents

        self.bprop = bprop

        self.name = name

    def __repr__(self):
        if self.op is None:
            return f"Var(value={self.value}, leaf)"
        else:
            return f"Var(value={self.value}, op={self.op.name})"


@dataclass
class Op:
    name: str
    num_inputs: int
    forward_fn: callable
    backward_fn: callable

    def __call__(self, *vars_in):
        assert len(vars_in) == self.num_inputs
        for v in vars_in:
            assert isinstance(v, Var), \
                f"Op {self.name} expected Var inputs, got {type(v)}"

        parent_vals = [v.value for v in vars_in]
        out_value = self.forward_fn(*parent_vals)

        def local_bprop(upstream):
            return self.backward_fn(parent_vals, upstream)

        return Var(
            value=out_value,
            op=self,
            parents=vars_in,
            bprop=local_bprop,
            name=self.name
        )

matmul = Op(
    name="matmul",
    num_inputs=2,
    forward_fn=lambda X, Y: X @ Y,
    backward_fn=lambda pvals, U: (
        U @ pvals[1].T,     # dL/dX
        pvals[0].T @ U      # dL/dY
    )
)

inner = Op(
    name="inner",
    num_inputs=2,
    forward_fn=lambda x, y: float(x.T @ y),
    backward_fn=lambda pvals, u: (
        u * pvals[1],       # dL/dx
        u * pvals[0]        # dL/dy
    )
)

x = Var([1.0, 2.0, 3.0], name="x")
y = Var([2.0, -1.0, 0.5], name="y")
A = Var([[2.0, 1.0, 0.0],
         [1.0, 3.0, 1.0],
         [0.0, 1.0, 2.0]], name="A")

# Forward pass graph:
z1 = matmul(A, x)      # z1 = A x
z2 = inner(z1, y)      # z2 = <z1, y>

print("z1 (forward):", z1.value)
print("z2 (forward):", z2.value)

# Backward pass:
upstream_z2 = 1.0
dz1, dy = z2.bprop(upstream_z2)
dA, dx = z1.bprop(dz1)

print("Gradient wrt z1:", dz1)
print("Gradient wrt y :", dy)
print("Gradient wrt A :", dA)
print("Gradient wrt x :", dx)

"""***2.3 Implementing the backpropagation***"""

import numpy as np
from dataclasses import dataclass

class Var:
    def __init__(self, value, op=None, parents=(), bprop=None, name=None):
        self.value = np.array(value)
        self.op = op
        self.parents = parents
        self.bprop = bprop
        self.name = name

    def __repr__(self):
        if self.op is None:
            return f"Var(name={self.name}, value={self.value}, leaf)"
        else:
            return f"Var(name={self.name}, value={self.value}, op={self.op.name})"


@dataclass
class Op:
    name: str
    num_inputs: int
    forward_fn: callable
    backward_fn: callable

    def __call__(self, *vars_in):
        assert len(vars_in) == self.num_inputs
        for v in vars_in:
            assert isinstance(v, Var), f"Op {self.name} expected Var, got {type(v)}"

        parent_vals = [v.value for v in vars_in]
        out_value = self.forward_fn(*parent_vals)

        def local_bprop(upstream):
            return self.backward_fn(parent_vals, upstream)

        return Var(
            value=out_value,
            op=self,
            parents=vars_in,
            bprop=local_bprop,
            name=self.name
        )


def _softmax(x):
    m = np.max(x)
    ex = np.exp(x - m)
    return ex / np.sum(ex)


add = Op(
    name="add",
    num_inputs=2,
    forward_fn=lambda x, y: x + y,
    backward_fn=lambda p, u: (u, u)
)

matmul = Op(
    name="matmul",
    num_inputs=2,
    forward_fn=lambda X, Y: X @ Y,
    backward_fn=lambda p, U: (
        U @ p[1].T,      # dL/dX
        p[0].T @ U       # dL/dY
    )
)

inner = Op(
    name="inner",
    num_inputs=2,
    forward_fn=lambda x, y: float(x.T @ y),
    backward_fn=lambda p, u: (
        u * p[1],        # dL/dx
        u * p[0]         # dL/dy
    )
)

solve = Op(
    name="solve",
    num_inputs=2,
    forward_fn=lambda A, x: np.linalg.solve(A, x),
    backward_fn=lambda p, u: (
        -(np.linalg.solve(p[0].T, u)[:, None] @ np.linalg.solve(p[0], p[1])[None, :]),
         np.linalg.solve(p[0].T, u)
    )
)

logdet = Op(
    name="logdet",
    num_inputs=1,
    forward_fn=lambda A: float(np.linalg.slogdet(A)[1]),
    backward_fn=lambda p, u: (u * np.linalg.inv(p[0]).T,)
)


def get_topological_order(output_node):
    order = []
    visited = set()

    def dfs(v):
        if id(v) in visited:
            return
        visited.add(id(v))
        for parent in v.parents:
            dfs(parent)
        order.append(v)

    dfs(output_node)
    return order


def backpropagation(output_node, input_nodes):
    topo = get_topological_order(output_node)
    grads = {}

    grads[id(output_node)] = np.ones_like(output_node.value, dtype=float)

    for node in reversed(topo):
        upstream = grads.get(id(node), None)
        if upstream is None:
            continue

        if node.op is None or node.bprop is None:
            continue

        parent_grads = node.bprop(upstream)

        for parent, g in zip(node.parents, parent_grads):
            if g is None:
                continue

            if id(parent) not in grads:
                grads[id(parent)] = np.zeros_like(parent.value, dtype=float)

            grads[id(parent)] = grads[id(parent)] + g

    return [grads.get(id(v), None) for v in input_nodes]



# Graph 1:
# z1 = matmul(A, x)
# z2 = inner(z1, y)

x = Var([1.0, 2.0, 3.0], name="x")
y = Var([2.0, -1.0, 0.5], name="y")
A = Var([[2.0, 1.0, 0.0],
         [1.0, 3.0, 1.0],
         [0.0, 1.0, 2.0]], name="A")

z1 = matmul(A, x);    z1.name = "z1"
z2 = inner(z1, y);    z2.name = "z2"

print("Graph 1 forward:")
print("z1 =", z1.value)
print("z2 =", z2.value)

grads_A_x_y = backpropagation(z2, [A, x, y])
print("\nGraph 1 backward grads:")
print("dL/dA =\n", grads_A_x_y[0])
print("dL/dx =\n", grads_A_x_y[1])
print("dL/dy =\n", grads_A_x_y[2])


# Graph 2:
# z_left  = solve(A, x)
# z_right = add(matmul(A, y), logdet(B))
# z2b     = inner(z_left, z_right)

B = Var([[1.0, 2.0, 3.0],
         [0.0, 1.0, 4.0],
         [5.0, 6.0, 0.0]], name="B")

z_left = solve(A, x);            z_left.name = "z_left"
tmp = matmul(A, y);              tmp.name = "A_y"
ldB = logdet(B);                 ldB.name = "ldB"
z_right = add(tmp, ldB);         z_right.name = "z_right"
z2b = inner(z_left, z_right);    z2b.name = "z2b"

print("\nGraph 2 forward:")
print("z_left  =", z_left.value)
print("z_right =", z_right.value)
print("z2b     =", z2b.value)

grads_A_B_x_y = backpropagation(z2b, [A, B, x, y])
print("\nGraph 2 backward grads:")
print("dL/dA =\n", grads_A_B_x_y[0])
print("dL/dB =\n", grads_A_B_x_y[1])
print("dL/dx =\n", grads_A_B_x_y[2])
print("dL/dy =\n", grads_A_B_x_y[3])

print("\nTopological order for graph 1 (z2):")
for node in get_topological_order(z2):
    print(node.name, "->", node.value)

print("\nTopological order for graph 2 (z2b):")
for node in get_topological_order(z2b):
    print(node.name, "->", node.value)

"""2.4"""

def grad(f):
    def grad_f(*raw_inputs):
        input_vars = []
        for i, val in enumerate(raw_inputs):
            v = Var(value=val, op=None, parents=(), bprop=None, name=f"in_{i}")
            input_vars.append(v)

        out_var = f(*input_vars)

        grads = backpropagation(out_var, input_vars)

        return grads

    return grad_f


def func_Test(x, y, A):
    left = solve(A, x)        # A^{-1} x
    right = matmul(A, y)      # A y
    out = inner(left, right)  # <A^{-1} x, A y>
    return out


grad_func_Test = grad(func_Test)

x_val = np.array([1.0, 2.0, 3.0])
y_val = np.array([2.0, -1.0, 0.5])
A_val = np.array([
    [2.0, 1.0, 0.0],
    [1.0, 3.0, 1.0],
    [0.0, 1.0, 2.0]
])

grads_x_y_A = grad_func_Test(x_val, y_val, A_val)

print("d func_Test / dx =\n", grads_x_y_A[0])
print("d func_Test / dy =\n", grads_x_y_A[1])
print("d func_Test / dA =\n", grads_x_y_A[2])

"""2.5"""

import numpy as np
from dataclasses import dataclass
class Var:
    def __init__(self, value, op=None, parents=(), bprop=None, name=None):
        self.value = np.array(value)
        self.op = op
        self.parents = parents
        self.bprop = bprop
        self.name = name

@dataclass
class Op:
    name: str
    num_inputs: int
    forward_fn: callable
    backward_fn: callable

    def __call__(self, *vars_in):
        assert len(vars_in) == self.num_inputs
        for v in vars_in:
            assert isinstance(v, Var)
        parent_vals = [v.value for v in vars_in]
        out_value = self.forward_fn(*parent_vals)

        def local_bprop(upstream):
            return self.backward_fn(parent_vals, upstream)

        return Var(
            value=out_value,
            op=self,
            parents=vars_in,
            bprop=local_bprop,
            name=self.name
        )

def _softmax(x):
    m = np.max(x)
    ex = np.exp(x - m)
    return ex / np.sum(ex)

add = Op(
    name="add", num_inputs=2, forward_fn=lambda x, y: x + y,
    backward_fn=lambda p, u: (u, u)
)
sub = Op(
    name="sub", num_inputs=2, forward_fn=lambda x, y: x - y,
    backward_fn=lambda p, u: (u, -u)
)
mul = Op(
    name="mul", num_inputs=2, forward_fn=lambda x, y: x * y,
    backward_fn=lambda p, u: (u * p[1], u * p[0])
)
inner = Op(
    name="inner", num_inputs=2, forward_fn=lambda x, y: float(x.T @ y),
    backward_fn=lambda p, u: (u * p[1], u * p[0])
)
solve = Op(
    name="solve", num_inputs=2, forward_fn=lambda A, x: np.linalg.solve(A, x),
    backward_fn=lambda p, u: (
        -(np.linalg.solve(p[0].T, u)[:, None] @ np.linalg.solve(p[0], p[1])[None, :]),
         np.linalg.solve(p[0].T, u)
    )
)
logdet = Op(
    name="logdet", num_inputs=1, forward_fn=lambda A: float(np.linalg.slogdet(A)[1]),
    backward_fn=lambda p, u: (u * np.linalg.inv(p[0]).T,)
)

def get_topological_order(output_node):
    order = []
    visited = set()
    def dfs(v):
        if id(v) in visited: return
        visited.add(id(v))
        for parent in v.parents: dfs(parent)
        order.append(v)
    dfs(output_node)
    return order

def backpropagation(output_node, input_nodes):
    topo = get_topological_order(output_node)
    grads = {}
    grads[id(output_node)] = np.ones_like(output_node.value, dtype=float)
    for node in reversed(topo):
        upstream = grads.get(id(node), None)
        if upstream is None or node.op is None or node.bprop is None: continue

        parent_grads = node.bprop(upstream)

        for parent, g in zip(node.parents, parent_grads):
            if g is None: continue
            if id(parent) not in grads:
                grads[id(parent)] = np.zeros_like(parent.value, dtype=float)
            grads[id(parent)] = grads[id(parent)] + g
    return [grads.get(id(v), None) for v in input_nodes]

def grad(f):
    def grad_f(*raw_inputs):
        input_vars = []
        for i, val in enumerate(raw_inputs):
            # Ensure only leaf nodes are tracked for gradient output
            if not isinstance(val, Var):
                v = Var(value=val, op=None, parents=(), bprop=None, name=f"in_{i}")
            else:
                v = val
            input_vars.append(v)

        out_var = f(*input_vars)
        grads = backpropagation(out_var, input_vars)
        return out_var.value, grads
    return grad_f

def gaussian_log_likelihood(X_var, mu_var, Sigma_var):
    N = X_var.value.shape[0]
    D = Sigma_var.value.shape[0]

    logdet_Sigma = logdet(Sigma_var)

    # D * log(2pi)
    const_term_val = D * np.log(2.0 * np.pi)
    const_term = Var(const_term_val, name="const_term")

    # term 1: -N/2 * log|2pi Sigma| = -N/2 * (D*log(2pi) + log|Sigma|)
    log_2piSigma = add(const_term, logdet_Sigma)
    scale1 = Var(-0.5 * N, name="scale1")
    term1 = mul(scale1, log_2piSigma)  # Var scalar

    # term 2: -1/2 * sum_n (x(n) - mu)^T Sigma^{-1} (x(n) - mu)
    quad_terms = []

    # Iterate over data points
    for n in range(N):
        # Create a new Var for each data point slice to track its dependency
        x_n = Var(X_var.value[n], name=f"x_{n}")
        diff_n = sub(x_n, mu_var)  # diff_n = x_n - mu
        Sigma_inv_diff = solve(Sigma_var, diff_n)  # Σ^{-1} (x_n - μ)
        quad_n = inner(diff_n, Sigma_inv_diff)     # (x_n - μ)^T Σ^{-1} (x_n - μ)
        quad_terms.append(quad_n)

    # Sum all quadratic terms
    if not quad_terms:
        quad_sum = Var(0.0)
    else:
        quad_sum = quad_terms[0]
        for q in quad_terms[1:]:
            quad_sum = add(quad_sum, q)

    scale2 = Var(-0.5, name="scale2")
    term2 = mul(scale2, quad_sum)

    # total log-likelihood = term1 + term2
    ll = add(term1, term2)
    return ll


gaussian_grad = grad(gaussian_log_likelihood)

# 1. Load the data from the .npy files
X_data = np.load('../report_src/X.npy')
mu_init = np.load('../report_src/mean.npy')
Sigma_init = np.load('../report_src/cov.npy')

print("Data loaded successfully.")

# 2. Compute log-likelihood and gradients
# Note: d_ll_dX is the gradient w.r.t the full X matrix, d_ll_dmu is w.r.t mu, d_ll_dSigma is w.r.t Sigma
loglik_value, (d_ll_dX, d_ll_dmu, d_ll_dSigma) = gaussian_grad(
    X_data,
    mu_init,
    Sigma_init
)

# 3. Print Results
print("\n" + "=" * 50)
print("Results for Question 2.5")
print("=" * 50)

print("Log-likelihood value:")
print(f"{loglik_value:.6f}")

print("\nGradient wrt mu (d/dμ):")
print(d_ll_dmu)

print("\nGradient wrt Sigma (d/dΣ):")
# Only displaying the relevant gradient components d_ll_dSigma and d_ll_dmu
print(d_ll_dSigma)
print("=" * 50)

"""3.3"""

import numpy as np
import matplotlib.pyplot as plt

np.random.seed(42)

w_star = np.array([3.0, -3.0])

# Distribution parameters
mu_x = np.array([1.0, 2.0])
Sigma_x = np.array([[3.0, 1.0],
                     [1.0, 2.0]])
sigma_epsilon = np.sqrt(3.0)

def generate_data(N, w_star, mu_x, Sigma_x, sigma_epsilon):
    # Sample x from multivariate normal
    X = np.random.multivariate_normal(mu_x, Sigma_x, N)

    # Sample epsilon from normal
    epsilon = np.random.normal(0, sigma_epsilon, N)

    # Generate y
    y = X @ w_star + epsilon

    return X, y

def linear_regression(X, y):
    """Perform linear regression: w = (X^T X)^{-1} X^T y"""
    w = np.linalg.solve(X.T @ X, X.T @ y)
    return w

def compute_true_risk(w, w_star, mu_x, Sigma_x, sigma_epsilon):
    """Compute true risk R(w) = E[(w^T x - y)^2]

    Since y = w_star^T x + epsilon, we have:
    R(w) = E[(w^T x - w_star^T x - epsilon)^2]
         = E[((w - w_star)^T x - epsilon)^2]
         = E[((w - w_star)^T x)^2] + E[epsilon^2]  (by independence)
         = (w - w_star)^T Sigma_x (w - w_star) + sigma_epsilon^2
    """
    diff = w - w_star
    risk = diff.T @ Sigma_x @ diff + sigma_epsilon**2
    return risk

def estimate_true_risk_empirical(w, w_star, mu_x, Sigma_x, sigma_epsilon, n_samples=10000):
    """Estimate true risk using a large synthetic dataset"""
    X_test, y_test = generate_data(n_samples, w_star, mu_x, Sigma_x, sigma_epsilon)
    predictions = X_test @ w
    risk = np.mean((predictions - y_test)**2)
    return risk

# Dataset sizes to test
N_values = [10, 100, 1000, 10000]
n_trials = 100

# Store results
results = {}

for N in N_values:
    print(f"\nProcessing N = {N}...")

    w_estimates = []
    risk_differences = []

    for trial in range(n_trials):
        # Generate dataset
        X, y = generate_data(N, w_star, mu_x, Sigma_x, sigma_epsilon)

        # Fit linear regression
        w_N = linear_regression(X, y)
        w_estimates.append(w_N)

        # Compute risk difference using empirical estimation
        R_w_N = estimate_true_risk_empirical(w_N, w_star, mu_x, Sigma_x, sigma_epsilon)
        R_w_star = estimate_true_risk_empirical(w_star, w_star, mu_x, Sigma_x, sigma_epsilon)
        risk_diff = R_w_N - R_w_star
        risk_differences.append(risk_diff)

    # Convert to arrays
    w_estimates = np.array(w_estimates)
    risk_differences = np.array(risk_differences)

    # Compute statistics
    w_errors = w_estimates - w_star
    empirical_mean = np.mean(w_errors, axis=0)
    empirical_cov = np.cov(w_errors.T)
    empirical_risk_diff_mean = np.mean(risk_differences)

    results[N] = {
        'w_errors': w_errors,
        'mean': empirical_mean,
        'cov': empirical_cov,
        'risk_diff_mean': empirical_risk_diff_mean,
        'risk_differences': risk_differences
    }

    # Print results
    print(f"\nResults for N = {N}:")
    print(f"Empirical mean of (w_N - w_*): {empirical_mean}")
    print(f"Empirical covariance of (w_N - w_*):\n{empirical_cov}")
    print(f"Empirical mean of R(w_N) - R(w_*): {empirical_risk_diff_mean:.6f}")

# Print summary table
print("\n" + "="*80)
print("SUMMARY TABLE")
print("="*80)
print(f"{'N':<10} {'Mean[w_N - w_*]':<30} {'R(w_N) - R(w_*)':<20}")
print("-"*80)
for N in N_values:
    mean_str = f"[{results[N]['mean'][0]:.4f}, {results[N]['mean'][1]:.4f}]"
    risk_str = f"{results[N]['risk_diff_mean']:.6f}"
    print(f"{N:<10} {mean_str:<30} {risk_str:<20}")
print("="*80)

print("\nCovariance matrices:")
for N in N_values:
    print(f"\nN = {N}:")
    print(results[N]['cov'])

"""3.4"""

import numpy as np
import matplotlib.pyplot as plt

# First, let's derive the theoretical predictions

# Given parameters
mu_x = np.array([1.0, 2.0])
Sigma_x = np.array([[3.0, 1.0],
                     [1.0, 2.0]])
sigma_epsilon_squared = 3.0
w_star = np.array([3.0, -3.0])

# Theoretical asymptotic distribution for sqrt(N) * (w_N - w_*)
# From theory: sqrt(N) * (w_N - w_*) -> N(0, sigma^2 * Sigma_x^{-1})
Sigma_x_inv = np.linalg.inv(Sigma_x)
theoretical_cov = sigma_epsilon_squared * Sigma_x_inv

print("Theoretical Results:")
print("="*80)
print(f"Theoretical covariance of sqrt(N) * (w_N - w_*): ")
print(theoretical_cov)
print(f"\nTheoretical mean of sqrt(N) * (w_N - w_*): [0, 0]")

# For the risk: N * (R(w_N) - R(w_*)) -> chi-squared distribution
# E[N * (R(w_N) - R(w_*))] -> sigma^2 * trace(Sigma_x^{-1} * Sigma_x) = sigma^2 * D
D = len(w_star)
theoretical_risk_limit = sigma_epsilon_squared * D
print(f"\nTheoretical limit of E[N * (R(w_N) - R(w_*))]: {theoretical_risk_limit}")
print("="*80)

# Now create plots comparing empirical to theoretical

# Dataset sizes
N_values = [10, 100, 1000, 10000]

# Extract empirical results (from previous code - results dictionary)
empirical_means_scaled = []
empirical_risk_scaled = []

for N in N_values:
    # Scale by sqrt(N) for parameter errors
    scaled_mean = np.sqrt(N) * results[N]['mean']
    empirical_means_scaled.append(scaled_mean)

    # Scale by N for risk differences
    scaled_risk = N * results[N]['risk_diff_mean']
    empirical_risk_scaled.append(scaled_risk)

empirical_means_scaled = np.array(empirical_means_scaled)
empirical_risk_scaled = np.array(empirical_risk_scaled)

# Create figure with subplots
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Plot 1: First component of sqrt(N) * (w_N - w_*) vs N
ax1 = axes[0, 0]
ax1.semilogx(N_values, empirical_means_scaled[:, 0], 'o-', label='Empirical', markersize=8, linewidth=2)
ax1.axhline(y=0, color='r', linestyle='--', label='Theoretical (0)', linewidth=2)
ax1.set_xlabel('N (log scale)', fontsize=12)
ax1.set_ylabel('Mean of √N * (w_N[0] - w_*[0])', fontsize=12)
ax1.set_title('First Component of Scaled Parameter Error', fontsize=13, fontweight='bold')
ax1.grid(True, alpha=0.3)
ax1.legend(fontsize=11)

# Plot 2: Second component of sqrt(N) * (w_N - w_*) vs N
ax2 = axes[0, 1]
ax2.semilogx(N_values, empirical_means_scaled[:, 1], 'o-', label='Empirical', markersize=8, linewidth=2, color='orange')
ax2.axhline(y=0, color='r', linestyle='--', label='Theoretical (0)', linewidth=2)
ax2.set_xlabel('N (log scale)', fontsize=12)
ax2.set_ylabel('Mean of √N * (w_N[1] - w_*[1])', fontsize=12)
ax2.set_title('Second Component of Scaled Parameter Error', fontsize=13, fontweight='bold')
ax2.grid(True, alpha=0.3)
ax2.legend(fontsize=11)

# Plot 3: N * (R(w_N) - R(w_*)) vs N
ax3 = axes[1, 0]
ax3.semilogx(N_values, empirical_risk_scaled, 'o-', label='Empirical', markersize=8, linewidth=2, color='green')
ax3.axhline(y=theoretical_risk_limit, color='r', linestyle='--',
            label=f'Theoretical ({theoretical_risk_limit:.1f})', linewidth=2)
ax3.set_xlabel('N (log scale)', fontsize=12)
ax3.set_ylabel('Mean of N * (R(w_N) - R(w_*))', fontsize=12)
ax3.set_title('Scaled Risk Difference', fontsize=13, fontweight='bold')
ax3.grid(True, alpha=0.3)
ax3.legend(fontsize=11)

# Plot 4: Comparison table visualization
ax4 = axes[1, 1]
ax4.axis('off')

# Create comparison table text
table_text = "Empirical vs Theoretical Comparison\n"
table_text += "="*60 + "\n\n"
table_text += f"Theoretical Covariance of √N*(w_N - w_*):\n"
table_text += f"  [[{theoretical_cov[0,0]:.4f}, {theoretical_cov[0,1]:.4f}],\n"
table_text += f"   [{theoretical_cov[1,0]:.4f}, {theoretical_cov[1,1]:.4f}]]\n\n"

for N in N_values:
    # Compute empirical covariance scaled by N
    empirical_cov_scaled = N * results[N]['cov']
    table_text += f"N = {N}:\n"
    table_text += f"  Empirical Cov (scaled by N):\n"
    table_text += f"    [[{empirical_cov_scaled[0,0]:.4f}, {empirical_cov_scaled[0,1]:.4f}],\n"
    table_text += f"     [{empirical_cov_scaled[1,0]:.4f}, {empirical_cov_scaled[1,1]:.4f}]]\n"
    table_text += f"  Risk: Emp={N * results[N]['risk_diff_mean']:.4f}, "
    table_text += f"Theory={theoretical_risk_limit:.4f}\n\n"

ax4.text(0.05, 0.95, table_text, transform=ax4.transAxes,
         fontsize=9, verticalalignment='top', fontfamily='monospace',
         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))

plt.tight_layout()
plt.savefig('../report_src/asymptotic_comparison.png', dpi=300, bbox_inches='tight')
#plt.show()

# Print detailed analysis
print("\n" + "="*80)
print("DETAILED COMPARISON WITH THEORY")
print("="*80)

print("\n1. Parameter Error Analysis:")
print("-"*80)
print("As N increases, √N * (w_N - w_*) should converge to N(0, σ²Σ_x^{-1})")
print(f"Theoretical mean: [0, 0]")
print(f"Theoretical covariance:\n{theoretical_cov}\n")

for N in N_values:
    scaled_cov = N * results[N]['cov']
    print(f"N = {N:5d}:")
    print(f"  Empirical mean of √N*(w_N - w_*): {np.sqrt(N) * results[N]['mean']}")
    print(f"  Empirical cov scaled by N:\n{scaled_cov}")
    print(f"  Frobenius distance from theory: {np.linalg.norm(scaled_cov - theoretical_cov):.4f}\n")

print("\n2. Risk Error Analysis:")
print("-"*80)
print(f"As N increases, N * (R(w_N) - R(w_*)) should converge to σ² * D = {theoretical_risk_limit}")
print()

for N in N_values:
    scaled_risk = N * results[N]['risk_diff_mean']
    deviation = abs(scaled_risk - theoretical_risk_limit)
    print(f"N = {N:5d}: Empirical = {scaled_risk:.4f}, "
          f"Theoretical = {theoretical_risk_limit:.4f}, "
          f"Deviation = {deviation:.4f}")

print("\n3. Discussion:")
print("-"*80)
print("• For small N (e.g., N=10), finite-sample effects cause deviations from")
print("  asymptotic theory. The empirical estimates are more variable.")
print()
print("• As N increases (100, 1000, 10000), the empirical distributions approach")
print("  the theoretical predictions, validating the asymptotic results.")
print()
print("• The scaled parameter errors √N*(w_N - w_*) should have mean close to 0")
print("  and covariance close to σ²Σ_x^{-1} for large N.")
print()
print("• The scaled risk N*(R(w_N) - R(w_*)) should converge to σ²*D = 6.0")
print("  as N → ∞, which we observe in the empirical results.")
print("="*80)

# Additional visualization: Distribution comparison for largest N
fig2, axes2 = plt.subplots(1, 2, figsize=(14, 5))

# For N = 10000, compare empirical distribution to theoretical
N_large = 10000
w_errors_scaled = np.sqrt(N_large) * results[N_large]['w_errors']

for idx, component in enumerate([0, 1]):
    ax = axes2[idx]

    # Histogram of empirical distribution
    ax.hist(w_errors_scaled[:, component], bins=30, density=True, alpha=0.6,
            label='Empirical', edgecolor='black')

    # Theoretical normal distribution
    theoretical_std = np.sqrt(theoretical_cov[component, component])
    x_range = np.linspace(w_errors_scaled[:, component].min(),
                          w_errors_scaled[:, component].max(), 100)
    theoretical_pdf = (1 / (theoretical_std * np.sqrt(2 * np.pi))) * \
                      np.exp(-0.5 * (x_range / theoretical_std)**2)
    ax.plot(x_range, theoretical_pdf, 'r-', linewidth=2, label='Theoretical N(0, σ²)')

    ax.set_xlabel(f'√N * (w_N[{component}] - w_*[{component}])', fontsize=12)
    ax.set_ylabel('Density', fontsize=12)
    ax.set_title(f'Distribution Comparison (N={N_large}, Component {component})',
                 fontsize=13, fontweight='bold')
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('../report_src/distribution_comparison.png', dpi=300, bbox_inches='tight')
#plt.show()

print("\nPlots saved: 'asymptotic_comparison.png' and 'distribution_comparison.png'")

"""7.1 - Final"""

import numpy as np
import matplotlib.pyplot as plt

def compute_cover_size_stable(epsilon, B, D):
    """
    Compute epsilon-cover size using log-space for numerical stability.
    |C| = max(1, (3*B*sqrt(D)/epsilon)^D)
    """
    # Work in log space to avoid overflow
    log_C = D * (np.log(3) + np.log(B) + 0.5*np.log(D) - np.log(epsilon))
    # Return max(1, exp(log_C))
    return np.maximum(1.0, np.exp(log_C))

def compute_bound(epsilon, N, K, B, D, delta):
    """
    Generalization bound: 2*K*epsilon + sqrt((1/2N)*log(2*|C|/delta))
    """
    C = compute_cover_size_stable(epsilon, B, D)

    # Approximation term
    term1 = 2 * K * epsilon

    # Statistical term - also use log-space for stability
    log_term = np.log(2) + np.log(C) - np.log(delta)
    term2 = np.sqrt(log_term / (2 * N))

    return term1 + term2

# Parameters
K, B, D, delta = 1.0, 10.0, 10.0, 0.01
N_values = [10, 100, 1000, 10000, 100000]

# Epsilon range (log-spaced)
epsilon_values = np.logspace(-6, 1, 500)

# Create plot
plt.figure(figsize=(12, 8))

for N in N_values:
    bounds = np.array([compute_bound(eps, N, K, B, D, delta)
                       for eps in epsilon_values])

    # Find and mark optimal epsilon
    min_idx = np.nanargmin(bounds)
    opt_eps = epsilon_values[min_idx]
    opt_bound = bounds[min_idx]

    plt.plot(epsilon_values, bounds, linewidth=2, label=f'N={N}')
    plt.plot(opt_eps, opt_bound, 'o', markersize=8)

plt.xscale('log')
plt.yscale('log')  # Log scale on y helps visualization
plt.xlabel('ε (epsilon) - Log Scale', fontsize=12, fontweight='bold')
plt.ylabel('Generalization Bound - Log Scale', fontsize=12, fontweight='bold')
plt.title('Generalization Bound vs ε for Different N', fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, which='both', alpha=0.3)
plt.tight_layout()

plt.savefig('../report_src/problem_7_1_generalization_bound.png', dpi=300, bbox_inches='tight')
#plt.show()

# Print optimal values
print("N        Optimal ε    Min Bound")
print("-" * 40)
for N in N_values:
    bounds = [compute_bound(eps, N, K, B, D, delta) for eps in epsilon_values]
    min_idx = np.argmin(bounds)
    print(f"{N:<8} {epsilon_values[min_idx]:<12.6f} {bounds[min_idx]:<10.6f}")

"""7.2 - Final Version"""

import numpy as np
import matplotlib.pyplot as plt

def compute_cover_size_stable(epsilon, B, D):
    log_C = D * (np.log(3) + np.log(B) + 0.5*np.log(D) - np.log(epsilon))
    return np.maximum(1.0, np.exp(log_C))

def compute_bound(epsilon, N, K, B, D, delta):
    C = compute_cover_size_stable(epsilon, B, D)
    term1 = 2 * K * epsilon
    log_term = np.log(2) + np.log(C) - np.log(delta)
    log_term = np.maximum(0, log_term)  # Safety check
    term2 = np.sqrt(log_term / (2 * N))
    return term1 + term2

def find_optimal_epsilon(N, K, B, D, delta):
    epsilon_values = np.logspace(-6, 1, 5000)  # Fine grid
    bounds = [compute_bound(eps, N, K, B, D, delta) for eps in epsilon_values]
    min_idx = np.argmin(bounds)
    return epsilon_values[min_idx], bounds[min_idx]

# Parameters
K, B, D, delta = 1.0, 10.0, 10.0, 0.01

# Sample N values (more than Gemini, less granular than Claude for clarity)
N_values = np.logspace(1, 5, 30)  # 30 points from 10 to 100,000
N_values = np.unique(np.round(N_values).astype(int))

# Find optimal epsilon for each N
print("Computing optimal epsilon for each N...")
optimal_epsilons = []
min_bounds = []

for N in N_values:
    opt_eps, min_bound = find_optimal_epsilon(N, K, B, D, delta)
    optimal_epsilons.append(opt_eps)
    min_bounds.append(min_bound)

optimal_epsilons = np.array(optimal_epsilons)
min_bounds = np.array(min_bounds)

# Print sample results
print("\nSample Results:")
print(f"{'N':<10} {'Optimal ε':<15} {'Min Bound':<15}")
print("-" * 40)
for i in [0, len(N_values)//4, len(N_values)//2, 3*len(N_values)//4, -1]:
    print(f"{N_values[i]:<10} {optimal_epsilons[i]:<15.6f} {min_bounds[i]:<15.6f}")

# Fit power law: ε* = C * N^α
log_N = np.log(N_values)
log_eps = np.log(optimal_epsilons)
coeffs = np.polyfit(log_N, log_eps, 1)
alpha = coeffs[0]
C_fit = np.exp(coeffs[1])

# Theoretical prediction
theoretical_alpha = -1.0 / D

print(f"\nPower Law Analysis:")
print(f"Empirical:    ε* ∝ N^{alpha:.6f}")
print(f"Theoretical:  ε* ∝ N^{theoretical_alpha:.6f}")
print(f"Difference:   {abs(alpha - theoretical_alpha):.6f}")

# Create comprehensive plot
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Plot 1: Log-log (main result)
ax1 = axes[0, 0]
ax1.loglog(N_values, optimal_epsilons, 'bo', markersize=6, label='Computed')
fitted_line = C_fit * N_values**alpha
ax1.loglog(N_values, fitted_line, 'r--', linewidth=2,
           label=f'Fit: ε* ∝ N^{alpha:.4f}')
theoretical_line = C_fit * N_values**theoretical_alpha
ax1.loglog(N_values, theoretical_line, 'g:', linewidth=2,
           label=f'Theory: ε* ∝ N^{-1/D:.4f}')
ax1.set_xlabel('N', fontsize=12, fontweight='bold')
ax1.set_ylabel('Optimal ε', fontsize=12, fontweight='bold')
ax1.set_title('Optimal ε vs N (Log-Log)', fontsize=13, fontweight='bold')
ax1.legend(fontsize=10)
ax1.grid(True, alpha=0.3, which='both')

# Plot 2: Semi-log (log N, linear ε)
ax2 = axes[0, 1]
ax2.semilogx(N_values, optimal_epsilons, 'go', markersize=6)
ax2.set_xlabel('N (log scale)', fontsize=12, fontweight='bold')
ax2.set_ylabel('Optimal ε', fontsize=12, fontweight='bold')
ax2.set_title('Semi-log Plot', fontsize=13, fontweight='bold')
ax2.grid(True, alpha=0.3, which='both')

# Plot 3: Minimum bound vs N
ax3 = axes[1, 0]
ax3.loglog(N_values, min_bounds, 'mo', markersize=6)
log_bound = np.log(min_bounds)
bound_coeffs = np.polyfit(log_N, log_bound, 1)
bound_alpha = bound_coeffs[0]
fitted_bounds = np.exp(bound_coeffs[1]) * N_values**bound_alpha
ax3.loglog(N_values, fitted_bounds, 'r--', linewidth=2,
           label=f'Fit: bound ∝ N^{bound_alpha:.4f}')
ax3.set_xlabel('N', fontsize=12, fontweight='bold')
ax3.set_ylabel('Minimum Bound', fontsize=12, fontweight='bold')
ax3.set_title('Minimum Bound vs N', fontsize=13, fontweight='bold')
ax3.legend(fontsize=10)
ax3.grid(True, alpha=0.3, which='both')

# Plot 4: Summary text
ax4 = axes[1, 1]
ax4.axis('off')
summary = f"""
SUMMARY

Power Law Fit:
  ε*(N) = {C_fit:.4f} × N^{alpha:.6f}

Theoretical:
  ε*(N) ∝ N^{-1/D} = N^{theoretical_alpha:.6f}

Error: {abs(alpha - theoretical_alpha)/abs(theoretical_alpha)*100:.2f}%

Interpretation:
- Log-log plot shows straight line
  → Confirms power law
- Slope ≈ {alpha:.4f} matches theory
- To halve ε, need N × {2**(1/abs(alpha)):.1f}
- Curse of dimensionality:
  exponent worsens as D increases
"""
ax4.text(0.1, 0.9, summary, transform=ax4.transAxes,
         fontsize=11, verticalalignment='top', fontfamily='monospace',
         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))

plt.tight_layout()
plt.savefig('../report_src/problem_7_2_optimal_epsilon.png', dpi=300, bbox_inches='tight')
